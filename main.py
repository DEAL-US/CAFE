from multiprocessing    import Process, Manager
from os.path            import isdir, isfile
from os                 import mkdir, remove
from shutil             import rmtree
from json               import loads

from settings           import N_THREADS, DATASET, PATH_TRAIN, PATH_RELS, MAX_CONTEXT_SIZE
from utils              import remove_negatives_train, join_files, filter_features, filter_contexts, split_file_rels, create_compressed_file
from worker_thread      import worker

import settings
import sys
import bz2

if __name__ == "__main__":  # This has to be explicitly done to launch processes
    print("=========================")
    print(DATASET)
    print("=========================")
    dset_output = f"output/{DATASET}/"

    if isdir(dset_output):
        if "--force" not in sys.argv:
            print("This dataset already has an output folder, remove it to proceed or use --force to overwrite")
            sys.exit()
        else:
            rmtree(dset_output)

    mkdir(dset_output)
    
    print("Removing negatives from the training split")
    remove_negatives_train(PATH_TRAIN)

    # Generate the csvs that contain the features associated to the triples
    processes = [Process(target=worker, args=(i,)) for i in range(N_THREADS)]
    for p in processes: p.start()
    for p in processes: p.join()

    if "--stop-after-generate" in sys.argv:
        sys.exit()

    # Join the csvs that have been generated by the threads
    print("Joining csvs")
    train_out = f"output/{DATASET}/train.csv"
    test_out = f"output/{DATASET}/test.csv"
    join_files(train_out, train_out + ".*")
    join_files(test_out, test_out + ".*")

    # Remove features that always have the same value and are thus useless
    print("Removing useless features")
    filter_features(train_out, test_out)

    rels_to_study = None
    rels_study_path = f"datasets/{DATASET}/relations_to_study.txt"
    if isfile(rels_study_path):
        rels_to_study = []
        with open(rels_study_path, "r") as f:
            for line in f:
                if line:
                    rels_to_study.append(line.strip().split("\t")[0])

    with open(PATH_RELS, "r") as f:
        rels = [x.strip().split("\t")[0] for x in f.readlines()]

    if rels_to_study is None:
        rels_to_study = rels

    # Split the csvs by relation to predict and context size
    print("Splitting csvs")
    rels_clean = [x.replace(" ", "_") for x in rels_to_study]

    mkdir(f"output/{DATASET}/train")
    mkdir(f"output/{DATASET}/test")

    for i in range(1, MAX_CONTEXT_SIZE + 1):
        print(f"Creating files for context size {i}")
        mkdir(f"output/{DATASET}/train/c{i}")
        mkdir(f"output/{DATASET}/test/c{i}")

        train_ctx_file = f"output/{DATASET}/train/c{i}/train.csv"
        test_ctx_file = f"output/{DATASET}/test/c{i}/test.csv"

        filter_contexts(i, train_out, train_ctx_file)
        filter_contexts(i, test_out, test_ctx_file)

        handles_train = {r: open(f"output/{DATASET}/train/c{i}/{r}_c{i}.csv", "w") 
                            for r in rels_clean}
        handles_test = {r: open(f"output/{DATASET}/test/c{i}/{r}_c{i}.csv", "w") 
                            for r in rels_clean}

        split_file_rels(train_ctx_file, handles_train)
        split_file_rels(test_ctx_file, handles_test)

        remove(train_ctx_file)
        remove(test_ctx_file)

        for handle in handles_train.values():
            handle.close()

        for handle in handles_test.values():
            handle.close()

        # Further remove more useless features
        for r in rels_clean:
            filter_features(
                f"output/{DATASET}/train/c{i}/{r}_c{i}.csv",
                f"output/{DATASET}/test/c{i}/{r}_c{i}.csv"
            )

    if "--compress" in sys.argv:
        create_compressed_file()